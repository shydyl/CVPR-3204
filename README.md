# More explanation for "TemMEGA: Temporal Masked Generative Modeling for Real-time Camera Movement Synthesis"

We apologize for taking up your time. Due to limitations on length and format, we provide additional links to further explain the issues in the paper. We sincerely appreciate your patience in reviewing it.

## PG3C: More analysis of the computational cost and runtime.

DanceCamera3D employs a diffusion model to generate camera movements, requiring multiple denoising steps during the testing phase to produce the final outputs. Under equivalent testing conditions, our method achieves a generation speed approximately **21 times faster** than DanceCamera3D, which utilizes DDIM as its sampler. Meanwhile, our method utilizes only **70% of the GPU memory** compared to DanceCamera3D.



## KRdU: Comparative experiment for various structural configurations of CME module.

We conduct ablation experiments on CME modules with different structures. In Case 1, the Causal Attention Mask is not utilized for feature processing. In Case 2, long-term memory is not used to enhance short-term memory; instead, both types of features are encoded directly. In Case 3, short-term memory is used to enhance long-term memory. Case 4 corresponds to the original results of our proposed TemMEGA model. The experimental results for these four cases are presented in the table below. From the results, we observe that both the Causal Attention Mask and the enhancement of short-term memory using long-term memory are effective components of the CME module. We will add more relevant experiments in the paper.

| Case | FID_k | Dist_k | DMR    | LCD   |
|------|-------|--------|--------|-------|
| 1    | 4.227 | 1.565  | 0.0049 | 0.182 |
| 2    | 4.205 | 1.559  | 0.0042 | 0.179 |
| 3    | 4.151 | 1.575  | 0.0038       | 0.177      |
| 4 (TemMEGA)    | 4.025 | 1.589  | 0.0035 | 0.177 |


## PG3C & crv9: Visual comparison with SOTA methods.

We perform a qualitative result analysis for TemMEGA and DanceCamera3D. Using the same dance sequence as input, we present the rendered videos in an online setting. The video is shown as follows:

<video src="https://github.com/shydyl/CVPR-3204/raw/main/offline.mp4"></video>

As observed, the results on the left, generated by DanceCamera3D, which utilizes a diffusion framework, lack coherence. In contrast, the results on the right, generated by TemMEGA, take temporal continuity into account, leading to significantly better smoothness and consistency compared to DanceCamera3D.

We also conducted a visual demonstration in the online setting. The video is shown below:

<video src="https://github.com/shydyl/CVPR-3204/raw/main/offline.mp4"></video>
![Vis_comparison](offline.mp4)

From the video, we can observe that TemMEGA generates camera movements that better align with the rhythm of the music and the dancer's movements compared to DanceCamera3D. Additionally, TemMEGA more effectively highlights the dancer's key movements and dynamic performance.